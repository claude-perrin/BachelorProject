{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/viktor/Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker-eu-central-1-224769526787\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/dataset\"\n",
    "\n",
    "# role = \"arn:aws:iam::235411143540:role/SageMakerRole\"# claude_perrin_3\n",
    "# role = \"arn:aws:iam::146258905784:role/SageMakerRole\"# claude_perrin_5\n",
    "role = \"arn:aws:iam::224769526787:role/SageMakerRole\"# claude_perrin_9\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-224769526787/cropped_train \n",
      " s3://sagemaker-eu-central-1-224769526787/cropped_train\n"
     ]
    }
   ],
   "source": [
    "# inputs = sagemaker_session.upload_data(path=\"saved_dataset\", bucket=bucket, key_prefix=prefix)\n",
    "# print(\"input spec (in this case, just an S3 path): {}\".format(inputs))\n",
    "# Train set and Test set\n",
    "# common_bucket_name = \"s3://sagemaker-eu-central-1-235411143540\"\n",
    "# train = (f\"{common_bucket_name}/train_directory\")\n",
    "# test = (f\"{common_bucket_name}/test_dir\" )\n",
    "\n",
    "common_bucket_name = f\"s3://{bucket}\"\n",
    "\n",
    "# train = (f\"{common_bucket_name}/crop_train_set\")\n",
    "# test = (f\"{common_bucket_name}/crop_test_set\")\n",
    "train = (f\"{common_bucket_name}/train_set\")\n",
    "train = (f\"{common_bucket_name}/cropped_train\")\n",
    "test = (f\"{common_bucket_name}/cropped_train\")\n",
    "print(train,'\\n',test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m#import sagemaker_containers\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mdist\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdistributed\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdetection\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfaster_rcnn\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m FastRCNNPredictor\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorchvision\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mops\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nms\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataset_helper\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m save_test_img, save_to_bucket, create_output_bucket\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdataset_helper\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_train_data_loader, get_test_data_loader\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mlr_scheduler\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m StepLR\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctional\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pad\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mconf\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_object_detection_model\u001b[39;49;00m(num_classes=NUMBER_OF_CLASSES):\u001b[37m\u001b[39;49;00m\n",
      "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    in_features = model.roi_heads.box_predictor.cls_score.in_features\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMain\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, args):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.args =  args\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.s3_session = boto3.client(\u001b[33m'\u001b[39;49;00m\u001b[33ms3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.image_size = (\u001b[34m704\u001b[39;49;00m,\u001b[34m704\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.rank = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.output_bucket_name = create_output_bucket(\u001b[36mself\u001b[39;49;00m.s3_session)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# self.output_bucket_name = \"vdidyk-test\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.use_cuda = args.num_gpus > \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.use_cuda \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.is_distributed = \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.args.hosts) > \u001b[34m1\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.args.backend \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.is_distributed:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.setup_distributed_system(args)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.rank = dist.get_rank()\u001b[37m\u001b[39;49;00m\n",
      "        torch.manual_seed(args.seed)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.use_cuda:\u001b[37m\u001b[39;49;00m\n",
      "            torch.cuda.manual_seed(args.seed)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msetup_distributed_system\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, args):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Initialize the distributed environment.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        world_size = \u001b[36mlen\u001b[39;49;00m(args.hosts)\u001b[37m\u001b[39;49;00m\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mWORLD_SIZE\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(world_size)\u001b[37m\u001b[39;49;00m\n",
      "        host_rank = args.hosts.index(args.current_host)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m==========SETUP DIST host_rank: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mhost_rank\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mRANK\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[36mstr\u001b[39;49;00m(host_rank)\u001b[37m\u001b[39;49;00m\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m==========SETUP DIST args.backend \u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.backend\u001b[33m}\u001b[39;49;00m\u001b[33m ; dist.get_world_size: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdist.get_world_size()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mrun\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        kwargs = {\u001b[33m\"\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mpin_memory\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[34mTrue\u001b[39;49;00m} \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.use_cuda \u001b[34melse\u001b[39;49;00m {}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[INFO] Traing loop variables:\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m is_distributed: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mself\u001b[39;49;00m.is_distributed\u001b[33m}\u001b[39;49;00m\u001b[33m; device: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mself\u001b[39;49;00m.device\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        train_loader = get_train_data_loader(\u001b[36mself\u001b[39;49;00m.args.batch_size, \u001b[36mself\u001b[39;49;00m.args.train_dir, is_distributed=\u001b[36mself\u001b[39;49;00m.is_distributed,  **kwargs)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[INFO] Train Loader: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtrain_loader\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        test_loader = get_test_data_loader(\u001b[36mself\u001b[39;49;00m.args.test_batch_size, \u001b[36mself\u001b[39;49;00m.args.test_dir, **kwargs)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[INFO] Test Loader: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtest_loader\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mlen(train_loader.sampler), len(train_loader.dataset) : \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_loader.sampler)\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_loader.dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        model = get_object_detection_model(num_classes=NUMBER_OF_CLASSES).to(\u001b[36mself\u001b[39;49;00m.device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.is_distributed \u001b[35mand\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.use_cuda:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# multi-machine multi-gpu case\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            model = torch.nn.parallel.DistributedDataParallel(model)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# single-machine multi-gpu case or single-machine or multi-machine cpu case\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            model = torch.nn.DataParallel(model)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        optimizer = optim.SGD(model.parameters(), lr=\u001b[36mself\u001b[39;49;00m.args.lr, momentum=\u001b[36mself\u001b[39;49;00m.args.momentum)\u001b[37m\u001b[39;49;00m\n",
      "        scheduler = StepLR(optimizer, step_size=\u001b[34m3\u001b[39;49;00m, gamma=\u001b[36mself\u001b[39;49;00m.args.gamma)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.args.epochs + \u001b[34m1\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mself\u001b[39;49;00m.train(model, train_loader, optimizer, epoch)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m (\u001b[36mself\u001b[39;49;00m.rank == \u001b[34m0\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mthere should have been test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# self.test(model, test_loader, epoch)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                model_prefix = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                model_path = \u001b[36mself\u001b[39;49;00m.save_model(model, \u001b[36mself\u001b[39;49;00m.args.model_dir, model_prefix)\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mself\u001b[39;49;00m.upload_model_to_s3(model_path, model_path)\u001b[37m\u001b[39;49;00m\n",
      "            scheduler.step()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# save model to s3\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model, train_loader, optimizer, epoch):\u001b[37m\u001b[39;49;00m\n",
      "        model.train()\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_idx, (data, targets,_) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_loader, \u001b[34m1\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m# Print bounding boxes for debugging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m batch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_idx\u001b[33m}\u001b[39;49;00m\u001b[33m  data: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdata\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m batch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_idx\u001b[33m}\u001b[39;49;00m\u001b[33m  targets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtargets\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            data = \u001b[36mlist\u001b[39;49;00m(image.to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m image \u001b[35min\u001b[39;49;00m data)\u001b[37m\u001b[39;49;00m\n",
      "            targets = [{k: v.to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m t.items()} \u001b[34mfor\u001b[39;49;00m t \u001b[35min\u001b[39;49;00m targets]\u001b[37m\u001b[39;49;00m\n",
      "            optimizer.zero_grad()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m batch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_idx\u001b[33m}\u001b[39;49;00m\u001b[33m  before model parse\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            output = model(data, targets)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m batch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_idx\u001b[33m}\u001b[39;49;00m\u001b[33m  output of the model: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00moutput\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "            loss = output[\u001b[33m\"\u001b[39;49;00m\u001b[33mloss_classifier\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "            loss.backward()\u001b[37m\u001b[39;49;00m\n",
      "            optimizer.step()\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mbreak\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model, test_loader, epoch):\u001b[37m\u001b[39;49;00m\n",
      "        model.eval()\u001b[37m\u001b[39;49;00m\n",
      "        test_loss = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        correct = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        correct_labels = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        total_samples = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        img_pathes = []\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m batch_id, (data, targets, original_image) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(test_loader, \u001b[34m1\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "            data = \u001b[36mlist\u001b[39;49;00m(image.to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m image \u001b[35min\u001b[39;49;00m data)\u001b[37m\u001b[39;49;00m\n",
      "            targets = [{k: v.to(\u001b[36mself\u001b[39;49;00m.device) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m t.items()} \u001b[34mfor\u001b[39;49;00m t \u001b[35min\u001b[39;49;00m targets]\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] targets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtargets\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m torch.no_grad():\u001b[37m\u001b[39;49;00m\n",
      "                prediction = model(data)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] after output: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprediction\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mfor\u001b[39;49;00m idx, (prediction_dict, target_dict) \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(prediction, targets)):\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] BEFORE _clean_targets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtarget_dict\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                target_dict = \u001b[36mself\u001b[39;49;00m._clean_targets(target_dict)\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m ] AFTER _clean_targets: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtarget_dict\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                prediction_dict = \u001b[36mself\u001b[39;49;00m._filter_prediction(predicted_dict=prediction_dict, max_predictions=\u001b[36mlen\u001b[39;49;00m(target_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] AFTER FILTERING prediction_dict: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprediction_dict\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                matching_labels = (prediction_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] == target_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]).sum().item()\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] matching_labels: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmatching_labels\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                total_samples += \u001b[34m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                correct += matching_labels\u001b[37m\u001b[39;49;00m\n",
      "                correct_labels += \u001b[36mlen\u001b[39;49;00m(target_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# box_loss = torch.nn.functional.smooth_l1_loss(prediction_dict['boxes'], target_dict['boxes'])\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# print(f\"=====[ epoch {epoch} test loop {batch_id} ] box_loss: {box_loss}\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# Combine the losses (you can adjust the weights based on your specific task)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# total_loss = box_loss\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# Accumulate the loss for the batch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# test_loss += total_loss.item()\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                test_loss = \u001b[34m0\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                prediction[idx] = prediction_dict\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[37m# Saving a frame with truth box on it!!\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop outside \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] idx: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00midx\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                name_prefix = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m-batch-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mstr\u001b[39;49;00m(batch_id)\u001b[33m}\u001b[39;49;00m\u001b[33m-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mstr\u001b[39;49;00m(target_dict[\u001b[33m'\u001b[39;49;00m\u001b[33midx\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[ epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m test loop outside \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] name_prefix: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mname_prefix\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                img_pathes.append(save_test_img(original_image[idx], prediction[idx], name_prefix))\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m===== epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m [test loop outside \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mbatch_id\u001b[33m}\u001b[39;49;00m\u001b[33m  ] img_pathes: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mimg_pathes\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mbreak\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        accuracy_labels = (correct / correct_labels) * \u001b[34m100\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        accuracy_boxes = (test_loss / total_samples)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m Accuracy Labels: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00maccuracy_labels\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m Loss Boxes: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00maccuracy_boxes\u001b[33m:\u001b[39;49;00m\u001b[33m.2f\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m%\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        save_to_bucket(img_pathes, \u001b[36mself\u001b[39;49;00m.output_bucket_name, \u001b[36mself\u001b[39;49;00m.s3_session)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_clean_targets\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, targets):\u001b[37m\u001b[39;49;00m\n",
      "        cleaned_targets = {}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Filter out invalid boxes\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        valid_boxes_mask = (targets[\u001b[33m'\u001b[39;49;00m\u001b[33mboxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].sum(axis=\u001b[34m1\u001b[39;49;00m) > \u001b[34m2\u001b[39;49;00m)    \u001b[37m\u001b[39;49;00m\n",
      "        valid_labels_mask = (targets[\u001b[33m'\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] != CLASSES_DICT[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackground\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        valid_area_mask = (targets[\u001b[33m'\u001b[39;49;00m\u001b[33marea\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] >= \u001b[34m1\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Combine all conditions using \"&\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        final_mask = valid_boxes_mask & valid_labels_mask & valid_area_mask\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(final_mask)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Apply the final mask\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m key, target_tensor \u001b[35min\u001b[39;49;00m targets.items():\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[34mif\u001b[39;49;00m key == \u001b[33m\"\u001b[39;49;00m\u001b[33midx\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "                cleaned_targets[key] = target_tensor\u001b[37m\u001b[39;49;00m\n",
      "                \u001b[34mcontinue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "            cleaned_targets[key] = target_tensor[final_mask]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m cleaned_targets\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_filter_prediction\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, predicted_dict, max_predictions, iou_threshold=\u001b[34m0.7\u001b[39;49;00m, confidence_threshold=\u001b[34m0.15\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
      "        predicted_boxes, scores = predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mboxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mscores\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        nms_indices = nms(predicted_boxes, scores, iou_threshold)\u001b[37m\u001b[39;49;00m\n",
      "        predicted_dict = {k: v[nms_indices] \u001b[34mfor\u001b[39;49;00m k,v \u001b[35min\u001b[39;49;00m predicted_dict.items()}\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# mask = predicted_dict[\"scores\"] >= confidence_threshold\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# predicted_dict = {k: v[mask] for k,v in predicted_dict.items()}\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# Limit the number of predictions\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        filtered_boxes, filtered_labels, filtered_scores = predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mboxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m],predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mscores\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(filtered_boxes) > max_predictions:\u001b[37m\u001b[39;49;00m\n",
      "            top_indices = torch.topk(filtered_scores, max_predictions).indices\u001b[37m\u001b[39;49;00m\n",
      "            predicted_dict = {k: v[top_indices] \u001b[34mfor\u001b[39;49;00m k,v \u001b[35min\u001b[39;49;00m predicted_dict.items()}\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(filtered_labels) < max_predictions:\u001b[37m\u001b[39;49;00m\n",
      "            n_to_pad = max_predictions - \u001b[36mlen\u001b[39;49;00m(filtered_labels)\u001b[37m\u001b[39;49;00m\n",
      "            predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = pad(filtered_labels, (\u001b[34m0\u001b[39;49;00m, n_to_pad), value=CLASSES_DICT[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackground\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mAFTER\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, predicted_dict[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m predicted_dict\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32msave_model\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model, model_dir, model_prefix):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving model: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m Saving to model_dir: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        path = os.path.join(model_dir, \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_prefix\u001b[33m}\u001b[39;49;00m\u001b[33m_model.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# recommended way from http://pytorch.org/docs/master/notes/serialization.html\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        torch.save(model.cpu().state_dict(), path)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m path\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mupload_model_to_s3\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, model_path, model_name):\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.s3_session.upload_file(model_path, \u001b[36mself\u001b[39;49;00m.output_bucket_name, model_name)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\u001b[37m\u001b[39;49;00m\n",
      "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    model = get_object_detection_model(num_classes=NUMBER_OF_CLASSES)\u001b[37m\u001b[39;49;00m\n",
      "    model = torch.nn.DataParallel(model)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(model_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\u001b[37m\u001b[39;49;00m\n",
      "        model.load_state_dict(torch.load(f))\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model.to(device)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser()\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[INFO] Started the job\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[INFO] torch version: \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, torch.__version__)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Data and model checkpoints directories\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m6\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for training (default: 64)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--test-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m6\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33minput batch size for testing (default: 1000)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m3\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mnumber of epochs to train (default: 10)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mLR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mlearning rate (default: 0.01)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mM\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSGD momentum (default: 0.5)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m, metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, help=\u001b[33m\"\u001b[39;49;00m\u001b[33mrandom seed (default: 1)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m100\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        metavar=\u001b[33m\"\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mhow many batches to wait before logging training status\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--backend\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34mNone\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mbackend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        default=\u001b[34m0.03\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mgamma parameter used by scheduler\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[37m\u001b[39;49;00m\n",
      "    )\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# # Container environment\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# print(\"SM_HOSTS: \",os.environ[\"SM_HOSTS\"])\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]))\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--hosts\", type=list, default=\"1\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--current-host\", type=str, default=\"1\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--model-dir\", type=str, default=\".\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--train-dir\", type=str, default=\"../cropped_train\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--test-dir\", type=str, default=\"../cropped_valid\")\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# parser.add_argument(\"--num-gpus\", type=int, default=0)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m=====[INFO] Running train loop\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    main_class = Main(parser.parse_args())\u001b[37m\u001b[39;49;00m\n",
      "    main_class.run()\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/faster_rcnn.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /opt/homebrew/share/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/viktor/Library/Application Support/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "instance_count = 2\n",
    "if instance_count > 1:\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} #torch_distributed\n",
    "else:\n",
    "    distribution = None\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir='.',        # Directory where your source code is located\n",
    "    role=role,\n",
    "    py_version=\"py310\",\n",
    "    framework_version=\"2.0.1\",\n",
    "    instance_count=instance_count,\n",
    "    # distribution=distribution,\n",
    "    instance_type=\"ml.m4.xlarge\", # ml.g4dn.xlarge ml.m4.xlarge\t\n",
    "    hyperparameters={\"epochs\": 2, \"backend\": \"gloo\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-12-08-13-49-17-721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-08 13:49:20 Starting - Starting the training job......\n",
      "2023-12-08 13:50:05 Starting - Preparing the instances for training......\n",
      "2023-12-08 13:51:20 Downloading - Downloading input data......\n",
      "2023-12-08 13:51:55 Training - Downloading the training image......\n",
      "2023-12-08 13:53:21 Uploading - Uploading generated training modelbash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-12-08 13:53:16,335 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-12-08 13:53:16,336 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:16,336 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:16,348 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-12-08 13:53:16,350 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-12-08 13:53:18,025 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:18,025 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:18,048 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:18,049 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:18,067 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:18,068 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:18,091 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-12-08-13-49-17-721\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-224769526787/pytorch-training-2023-12-08-13-49-17-721/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"backend\":\"gloo\",\"epochs\":2}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=4\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-eu-central-1-224769526787/pytorch-training-2023-12-08-13-49-17-721/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2023-12-08-13-49-17-721\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-224769526787/pytorch-training-2023-12-08-13-49-17-721/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"2\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_BACKEND=gloo\n",
      "SM_HP_EPOCHS=2\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.10 train.py --backend gloo --epochs 2\n",
      "2023-12-08 13:53:18,132 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 15, in <module>\n",
      "from training.dataset_helper import save_test_img, save_to_bucket, create_output_bucket, get_train_data_loader, get_test_data_loader\n",
      "ModuleNotFoundError: No module named 'training'\n",
      "bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-12-08 13:53:16,517 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-12-08 13:53:16,518 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:16,518 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:16,530 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-12-08 13:53:16,533 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-12-08 13:53:18,244 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:18,244 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:18,258 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:18,258 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:18,274 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "2023-12-08 13:53:18,275 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-12-08 13:53:18,288 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m4.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-12-08-13-49-17-721\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-224769526787/pytorch-training-2023-12-08-13-49-17-721/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.m4.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m4.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"backend\":\"gloo\",\"epochs\":2}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-2\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.m4.xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=4\n",
      "SM_NUM_GPUS=0\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-eu-central-1-224769526787/pytorch-training-2023-12-08-13-49-17-721/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.m4.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-training-2023-12-08-13-49-17-721\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-224769526787/pytorch-training-2023-12-08-13-49-17-721/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.m4.xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m4.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"2\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_BACKEND=gloo\n",
      "SM_HP_EPOCHS=2\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.10 train.py --backend gloo --epochs 2\n",
      "2023-12-08 13:53:18,320 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/code/train.py\", line 15, in <module>\n",
      "from training.dataset_helper import save_test_img, save_to_bucket, create_output_bucket, get_train_data_loader, get_test_data_loader\n",
      "ModuleNotFoundError: No module named 'training'\n",
      "2023-12-08 13:53:20,339 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-12-08 13:53:20,339 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "2023-12-08 13:53:20,340 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2023-12-08 13:53:20,340 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"ModuleNotFoundError: No module named 'training'\"\n",
      "Command \"/opt/conda/bin/python3.10 train.py --backend gloo --epochs 2\"\n",
      "2023-12-08 13:53:20,340 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "2023-12-08 13:53:20,572 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-12-08 13:53:20,573 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 1 from exiting process.\n",
      "2023-12-08 13:53:20,573 sagemaker-training-toolkit ERROR    Reporting training FAILURE\n",
      "2023-12-08 13:53:20,573 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "ExitCode 1\n",
      "ErrorMessage \"ModuleNotFoundError: No module named 'training'\"\n",
      "Command \"/opt/conda/bin/python3.10 train.py --backend gloo --epochs 2\"\n",
      "2023-12-08 13:53:20,573 sagemaker-training-toolkit ERROR    Encountered exit_code 1\n",
      "\n",
      "2023-12-08 13:53:32 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job pytorch-training-2023-12-08-13-49-17-721: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"ModuleNotFoundError: No module named 'training'\"\nCommand \"/opt/conda/bin/python3.10 train.py --backend gloo --epochs 2\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/viktor/polsl/bachelor_project/sagemaker/code/training/sagemaker_train.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/viktor/polsl/bachelor_project/sagemaker/code/training/sagemaker_train.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m estimator\u001b[39m.\u001b[39;49mfit({\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m: train, \u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m: test})\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:311\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[39mreturn\u001b[39;00m context\n\u001b[1;32m    309\u001b[0m     \u001b[39mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m run_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/estimator.py:1315\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobs\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1314\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 1315\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlatest_training_job\u001b[39m.\u001b[39;49mwait(logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/estimator.py:2598\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2596\u001b[0m \u001b[39m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[1;32m   2597\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mNone\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2598\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_session\u001b[39m.\u001b[39;49mlogs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjob_name, wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, log_type\u001b[39m=\u001b[39;49mlogs)\n\u001b[1;32m   2599\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2600\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39mwait_for_job(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_name)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/session.py:4973\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogs_for_job\u001b[39m(\u001b[39mself\u001b[39m, job_name, wait\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, poll\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, log_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAll\u001b[39m\u001b[39m\"\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   4953\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[1;32m   4954\u001b[0m \n\u001b[1;32m   4955\u001b[0m \u001b[39m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4971\u001b[0m \u001b[39m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[1;32m   4972\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4973\u001b[0m     _logs_for_job(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mboto_session, job_name, wait, poll, log_type, timeout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/session.py:6897\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6894\u001b[0m             last_profiler_rule_statuses \u001b[39m=\u001b[39m profiler_rule_statuses\n\u001b[1;32m   6896\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[0;32m-> 6897\u001b[0m     _check_job_status(job_name, description, \u001b[39m\"\u001b[39;49m\u001b[39mTrainingJobStatus\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   6898\u001b[0m     \u001b[39mif\u001b[39;00m dot:\n\u001b[1;32m   6899\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/session.py:6950\u001b[0m, in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6944\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCapacityError\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(reason):\n\u001b[1;32m   6945\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mCapacityError(\n\u001b[1;32m   6946\u001b[0m         message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6947\u001b[0m         allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6948\u001b[0m         actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6949\u001b[0m     )\n\u001b[0;32m-> 6950\u001b[0m \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   6951\u001b[0m     message\u001b[39m=\u001b[39mmessage,\n\u001b[1;32m   6952\u001b[0m     allowed_statuses\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mCompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStopped\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   6953\u001b[0m     actual_status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m   6954\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job pytorch-training-2023-12-08-13-49-17-721: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"ModuleNotFoundError: No module named 'training'\"\nCommand \"/opt/conda/bin/python3.10 train.py --backend gloo --epochs 2\", exit code: 1"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"train\": train, \"test\": test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-eu-central-1-235411143540/pytorch-training-2023-10-27-19-14-02-631/output/model.tar.gz), script artifact (s3://sagemaker-eu-central-1-235411143540/pytorch-training-2023-10-27-19-14-02-631/source/sourcedir.tar.gz), and dependencies ([]) into single tar.gz file located at s3://sagemaker-eu-central-1-235411143540/pytorch-training-2023-10-27-19-49-51-655/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-training-2023-10-27-19-49-51-655\n",
      "INFO:sagemaker:Creating endpoint-config with name pytorch-training-2023-10-27-19-49-51-655\n",
      "INFO:sagemaker:Creating endpoint with name pytorch-training-2023-10-27-19-49-51-655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "data_dir = \"data/MNIST/raw\"\n",
    "with gzip.open(os.path.join(data_dir, \"t10k-images-idx3-ubyte.gz\"), \"rb\") as f:\n",
    "    images = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1, 28, 28).astype(np.float32)\n",
    "\n",
    "mask = random.sample(range(len(images)), 16)  # randomly select some of the test images\n",
    "mask = np.array(mask, dtype=np.int32)\n",
    "data = images[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw prediction result:\n",
      "[[    0.         -1531.03552246  -775.55126953  -882.36639404\n",
      "  -1270.45324707  -764.815979    -803.70916748 -1069.17114258\n",
      "   -846.71520996 -1144.79272461]\n",
      " [ -355.86022949  -440.81268311  -202.45556641  -266.35440063\n",
      "   -401.78820801  -287.02746582  -264.79299927  -576.97924805\n",
      "      0.          -450.95666504]\n",
      " [ -800.54821777  -702.08032227  -719.37127686  -437.40423584\n",
      "    -19.32922363  -343.06335449  -536.19616699  -357.53234863\n",
      "   -276.92120361     0.        ]\n",
      " [ -645.06445312  -817.94506836  -521.99169922  -694.60449219\n",
      "      0.          -513.83740234  -381.3157959   -398.30310059\n",
      "   -555.9921875   -121.46380615]\n",
      " [ -357.71896362  -962.51599121 -1015.74230957  -680.15093994\n",
      "   -666.33557129     0.          -534.38995361  -856.23693848\n",
      "   -422.42626953  -573.88012695]\n",
      " [ -819.02593994 -1104.11669922  -689.6842041  -1209.65405273\n",
      "   -659.09802246  -546.53356934     0.         -1378.28796387\n",
      "   -805.45489502 -1012.93432617]\n",
      " [ -484.87252808  -614.40661621  -453.84750366  -701.50537109\n",
      "    -93.70651245  -325.65887451     0.          -732.47674561\n",
      "   -498.28503418  -366.34851074]\n",
      " [ -864.89367676  -563.41149902  -569.86663818     0.\n",
      "  -1001.46813965  -456.31408691  -927.51104736  -788.0723877\n",
      "   -554.10241699  -804.78619385]\n",
      " [ -218.83981323  -355.72155762     0.          -332.71469116\n",
      "   -321.63140869  -390.01821899  -124.10774231  -432.21386719\n",
      "   -195.24407959  -392.17584229]\n",
      " [ -641.00421143  -104.61210632  -388.60668945  -253.2147522\n",
      "   -287.53543091  -302.73629761  -583.36999512     0.\n",
      "    -24.7250824    -31.34117126]\n",
      " [ -783.61767578     0.          -288.21511841  -504.97497559\n",
      "   -528.53430176  -625.63964844  -542.39672852  -543.38818359\n",
      "   -302.31768799  -580.06860352]\n",
      " [ -821.30474854  -670.8548584   -562.62127686  -288.82348633\n",
      "   -163.99331665  -445.56707764  -641.06140137  -225.40892029\n",
      "   -390.40707397     0.        ]\n",
      " [ -882.57043457  -993.15258789  -746.38568115  -802.28155518\n",
      "      0.          -736.19134521  -638.42144775  -594.53948975\n",
      "   -628.91894531  -274.07662964]\n",
      " [ -347.95822144  -978.96801758  -751.68457031  -612.77264404\n",
      "   -616.88000488     0.          -203.94094849  -927.66607666\n",
      "   -529.12774658  -632.15087891]\n",
      " [-1017.86151123     0.          -572.97351074  -565.74188232\n",
      "   -566.73834229  -673.61761475  -687.50708008  -547.92102051\n",
      "   -563.53790283  -585.07092285]\n",
      " [-1166.64440918 -1218.38293457 -1108.50622559 -1145.75036621\n",
      "      0.          -847.92321777  -759.57318115  -737.06713867\n",
      "   -776.37811279  -323.097229  ]]\n",
      "\n",
      "Labeled predictions: \n",
      "[(0, 0.0), (1, -1531.0355224609375), (2, -775.55126953125), (3, -882.3663940429688), (4, -1270.4532470703125), (5, -764.8159790039062), (6, -803.7091674804688), (7, -1069.171142578125), (8, -846.7152099609375), (9, -1144.792724609375)]\n",
      "\n",
      "Most likely answer: (0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(np.expand_dims(data, axis=1))\n",
    "print(\"Raw prediction result:\")\n",
    "print(response)\n",
    "print()\n",
    "\n",
    "labeled_predictions = list(zip(range(10), response[0]))\n",
    "print(\"Labeled predictions: \")\n",
    "print(labeled_predictions)\n",
    "print()\n",
    "\n",
    "labeled_predictions.sort(key=lambda label_and_prob: 1.0 - label_and_prob[1])\n",
    "print(\"Most likely answer: {}\".format(labeled_predictions[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: pytorch-training-2023-10-27-19-49-51-655\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"pytorch-training-2023-10-27-19-49-51-655\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/viktor/polsl/bachelor_project/sagemaker/main.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/viktor/polsl/bachelor_project/sagemaker/main.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sagemaker_session\u001b[39m.\u001b[39;49mdelete_endpoint(endpoint_name\u001b[39m=\u001b[39;49mpredictor\u001b[39m.\u001b[39;49mendpoint_name)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/sagemaker/session.py:4238\u001b[0m, in \u001b[0;36mSession.delete_endpoint\u001b[0;34m(self, endpoint_name)\u001b[0m\n\u001b[1;32m   4232\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Delete an Amazon SageMaker ``Endpoint``.\u001b[39;00m\n\u001b[1;32m   4233\u001b[0m \n\u001b[1;32m   4234\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   4235\u001b[0m \u001b[39m    endpoint_name (str): Name of the Amazon SageMaker ``Endpoint`` to delete.\u001b[39;00m\n\u001b[1;32m   4236\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4237\u001b[0m LOGGER\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mDeleting endpoint with name: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, endpoint_name)\n\u001b[0;32m-> 4238\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msagemaker_client\u001b[39m.\u001b[39;49mdelete_endpoint(EndpointName\u001b[39m=\u001b[39;49mendpoint_name)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/botocore/client.py:535\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 535\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/botocore/client.py:980\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    979\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 980\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the DeleteEndpoint operation: Could not find endpoint \"pytorch-training-2023-10-27-19-49-51-655\"."
     ]
    }
   ],
   "source": [
    "sagemaker_session.delete_endpoint(endpoint_name=predictor.endpoint_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
