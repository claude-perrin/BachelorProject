{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import nms\n",
    "from torchvision.ops.boxes import box_convert,box_iou\n",
    "\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from dataset import ObjectDetectionDataset\n",
    "from dataset_helper import get_train_data_loader\n",
    "from conf import *\n",
    "\n",
    "torch.manual_seed = 0\n",
    "\n",
    "model_path = \"../models/epoch-4_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_object_detection_model(num_classes=NUMBER_OF_CLASSES):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    state_dict= torch.load(model_path)\n",
    "    updated_state = {k.replace(\"module.\", \"\"): v for k,v in state_dict.items()}\n",
    "    model.load_state_dict(updated_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "def save_test_img(img, target, prefix):\n",
    "    # img = img.permute(2,0,1).cpu().numpy()  # Convert to (height, width, channels)\n",
    "    img = img.cpu().numpy()  # Convert to (height, width, channels)\n",
    "\n",
    "    # img = img.astype('uint8')\n",
    "    # img = img\n",
    "    # Draw bounding boxes on the image\n",
    "    print(target)\n",
    "    for box, label in zip(target['boxes'], target['labels']):\n",
    "        x, y, w, h = box.tolist()\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        box_color = BOX_COLOR[label.item()]\n",
    "        cv2.rectangle(img, (x, y), (w, h), box_color, 2)\n",
    "\n",
    "    # Save the image with bounding boxes\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), 'test_output')):\n",
    "        os.makedirs(os.path.join(os.getcwd(), 'test_output'))\n",
    "    # cv2.imshow(img)\n",
    "    img_path = f\"./test_output/output_image_{prefix}.png\"\n",
    "    cv2.imwrite(img_path, img)\n",
    "    return img_path\n",
    "\n",
    "def clean_targets(targets):\n",
    "    cleaned_targets = {}\n",
    "\n",
    "    \n",
    "    # Filter out invalid boxes\n",
    "    valid_boxes_mask = (targets['boxes'].sum(axis=1) > 2)    \n",
    "    valid_labels_mask = (targets['labels'] != CLASSES_TO_IDX[\"background\"])\n",
    "    valid_area_mask = (targets['area'] >= 1)\n",
    "\n",
    "    # Combine all conditions using \"&\"\n",
    "    final_mask = valid_boxes_mask & valid_labels_mask & valid_area_mask\n",
    "    # Apply the final mask\n",
    "    for key, target_tensor in targets.items():\n",
    "        if key == \"idx\":\n",
    "            cleaned_targets[key] = target_tensor\n",
    "            continue\n",
    "        cleaned_targets[key] = target_tensor[final_mask]\n",
    "    return cleaned_targets\n",
    "\n",
    "\n",
    "def inference_filter_prediction(output, iou_threshold=0.25, confidence_threshold=0.50):\n",
    "\n",
    "    cleaned_output = []\n",
    "    for predicted_dict in output:\n",
    "        mask = predicted_dict[\"scores\"] >= confidence_threshold\n",
    "        predicted_dict = {k: v[mask] for k,v in predicted_dict.items()}\n",
    "        predicted_boxes, scores = predicted_dict[\"boxes\"], predicted_dict[\"scores\"]\n",
    "        nms_indices = nms(predicted_boxes, scores, iou_threshold)\n",
    "        print(nms_indices)\n",
    "        predicted_dict = {k: v[nms_indices] for k,v in predicted_dict.items()}\n",
    "\n",
    "        cleaned_output.append(predicted_dict)\n",
    "    return cleaned_output\n",
    "\n",
    "def get_images(image_path):\n",
    "        # reading the images and converting them to correct size and color\n",
    "        original_image = cv2.imread(image_path)\n",
    "        grayscale = to_grayscale(original_image)\n",
    "        grayscale = normalize_image(grayscale)\n",
    "        grayscale = torch.from_numpy(grayscale).float()\n",
    "        grayscale = grayscale.unsqueeze(0)\n",
    "        \n",
    "        return grayscale, torch.from_numpy(original_image)\n",
    "\n",
    "def to_grayscale(image):\t\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def normalize_image(img):\n",
    "    return img / 255\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader, 1):\n",
    "        data = list(image.to(device) for image in data)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, targets)\n",
    "        print(f\"=====[ epoch {epoch} batch {batch_idx}  output of the model: {output}\")\n",
    "\n",
    "        loss = output[\"loss_classifier\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def run(model, train_loader):\n",
    "    epochs = 2\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_loader, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/mlp/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = get_object_detection_model()\n",
    "# dataset = get_train_data_loader(1, \"../die\")\n",
    "# print(model)\n",
    "# run(model, dataset)\n",
    "model.eval()\n",
    "original_image_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 24, 25, 27, 30, 31, 32, 33, 34, 35, 36, 38, 40])\n",
      "detections-331\n",
      "{'boxes': tensor([[179.7976, 457.4123, 210.7576, 485.2520],\n",
      "        [204.6704, 493.8415, 239.8764, 523.7800],\n",
      "        [240.7986, 588.1495, 324.5114, 668.8118],\n",
      "        [525.4587, 544.5194, 580.9304, 576.6167],\n",
      "        [724.6764, 656.5187, 870.8930, 714.3874],\n",
      "        [780.2134, 678.4894, 905.0087, 744.9536],\n",
      "        [481.3517, 504.8663, 528.1044, 535.6287],\n",
      "        [519.5519, 503.5792, 561.9777, 532.4686],\n",
      "        [662.1407, 543.5578, 725.0336, 577.6752],\n",
      "        [474.0178, 537.5209, 535.0876, 576.2452],\n",
      "        [829.0297, 591.0197, 922.8293, 633.6086],\n",
      "        [598.9414, 497.6663, 641.1106, 528.5678],\n",
      "        [569.0800, 540.7796, 629.8018, 572.8284],\n",
      "        [907.9136, 624.7111, 959.6132, 664.7378],\n",
      "        [562.5197, 498.9753, 607.6439, 522.8589],\n",
      "        [783.0757, 544.1405, 840.8433, 577.8701],\n",
      "        [375.2706, 557.3547, 401.8450, 576.9027],\n",
      "        [712.0106, 543.6039, 759.3523, 568.7767],\n",
      "        [691.7926, 639.7725, 816.0807, 684.7418],\n",
      "        [880.2307, 549.0178, 921.2689, 576.1431],\n",
      "        [901.6576, 549.0978, 955.2668, 579.9666],\n",
      "        [753.8190, 545.7408, 798.6385, 580.5710],\n",
      "        [583.1611, 569.2332, 667.9228, 601.8032],\n",
      "        [829.4694, 545.3920, 880.8758, 572.5798],\n",
      "        [788.3765, 505.4283, 837.6827, 542.5385],\n",
      "        [893.2736, 591.9196, 958.9002, 627.1399],\n",
      "        [606.0754, 583.4199, 710.1104, 624.3416],\n",
      "        [733.6377, 503.9913, 771.7538, 529.9612],\n",
      "        [388.1898, 495.8368, 399.2309, 517.3864],\n",
      "        [678.4166, 506.3128, 714.2904, 529.3448],\n",
      "        [656.2020, 608.7603, 755.8323, 648.5497],\n",
      "        [638.7411, 501.2593, 689.3314, 527.0174],\n",
      "        [648.6857, 642.5672, 675.5467, 695.9864],\n",
      "        [368.1942, 412.4091, 387.8131, 427.3266],\n",
      "        [390.2901, 583.5056, 415.9275, 633.0324]], grad_fn=<IndexBackward0>), 'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 3, 4, 4, 4, 3, 4, 3]), 'scores': tensor([0.9356, 0.9340, 0.9333, 0.9239, 0.9215, 0.9183, 0.9107, 0.9052, 0.9049,\n",
      "        0.9021, 0.9010, 0.8905, 0.8877, 0.8872, 0.8843, 0.8815, 0.8742, 0.8711,\n",
      "        0.8526, 0.8518, 0.8459, 0.8140, 0.8013, 0.7870, 0.7686, 0.7559, 0.7387,\n",
      "        0.7375, 0.7085, 0.6975, 0.6898, 0.6638, 0.6621, 0.6162, 0.5910],\n",
      "       grad_fn=<IndexBackward0>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./test_output/output_image_detections-331.png'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir = \"../die/street2.jpg\"\n",
    "grayscale, image = get_images(image_dir)\n",
    "grayscale = grayscale.unsqueeze(0)\n",
    "\n",
    "output = model(grayscale)\n",
    "output = inference_filter_prediction(output)\n",
    "# print(\"OUTPUT: \", output)\n",
    "it = random.randint(0, 1000)\n",
    "prefix=f\"detections-{it}\"\n",
    "print(prefix)\n",
    "# output = clean_targets(targets[0])\n",
    "save_test_img(image, output[0], prefix)\n",
    "# x = image[0].permute(2,1,0).numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
