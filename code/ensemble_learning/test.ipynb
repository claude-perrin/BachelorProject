{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch\n",
    "from torchvision.ops import nms\n",
    "from torchvision.ops.boxes import box_convert,box_iou\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2 \n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn \n",
    "import numpy as np\n",
    "from torchvision.models.detection import ssd300_vgg16, SSD300_VGG16_Weights, ssd\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from dataset import ObjectDetectionDataset\n",
    "# from dataset_helper import get_train_data_loader\n",
    "from conf import *\n",
    "import datetime\n",
    "torch.manual_seed = 0\n",
    "\n",
    "model_path1 = \"models/FasterRcnn_V1_epoch-6_model.pth\"\n",
    "model_path2 = \"models/FasterRcnn_V2_epoch-4_model.pth\"\n",
    "model_path3 = \"models/SSD_epoch-8_model.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model1(num_classes=NUMBER_OF_CLASSES):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    state_dict= torch.load(model_path1)\n",
    "    updated_state = {k.replace(\"module.\", \"\"): v for k,v in state_dict.items()}\n",
    "    model.load_state_dict(updated_state)\n",
    "    print(f\"MODEL from volume: {model_path1} is loaded successfuly\")\n",
    "    return model\n",
    "\n",
    "def load_model2(num_classes=NUMBER_OF_CLASSES):\n",
    "    model = fasterrcnn_resnet50_fpn_v2(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    state_dict= torch.load(model_path2)\n",
    "    updated_state = {k.replace(\"module.\", \"\"): v for k,v in state_dict.items()}\n",
    "    model.load_state_dict(updated_state)\n",
    "    print(f\"MODEL from volume: {model_path2} is loaded successfuly\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ssd_detection_model(num_classes=NUMBER_OF_CLASSES):\n",
    "    ssd_model = ssd300_vgg16(weights=False)\n",
    "    num_anchors = ssd_model.anchor_generator.num_anchors_per_location()\n",
    "    out_channels = [512,1024,512,256,256,256]\n",
    "    ssd_model.head = ssd.SSDHead(out_channels, num_anchors, num_classes+1)\n",
    "    state_dict= torch.load(model_path3)\n",
    "    updated_state = {k.replace(\"module.\", \"\"): v for k,v in state_dict.items()}\n",
    "    ssd_model.load_state_dict(updated_state)\n",
    "    print(f\"MODEL from volume: {model_path3} is loaded successfuly\")\n",
    "\n",
    "    return ssd_model\n",
    "\n",
    "\n",
    "def save_test_img(img, target, prefix):\n",
    "    # img = img.permute(2,0,1).cpu().numpy()  # Convert to (height, width, channels)\n",
    "    img = img.cpu().numpy()  # Convert to (height, width, channels)\n",
    "\n",
    "    # img = img.astype('uint8')\n",
    "    # img = img\n",
    "    # Draw bounding boxes on the image\n",
    "    print(target)\n",
    "    for box, label in zip(target['boxes'], target['labels']):\n",
    "        x, y, w, h = box.tolist()\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        # box_color = BOX_COLOR[label.item()]\n",
    "        print(label)\n",
    "        box_color = (255,255,255)\n",
    "        cv2.rectangle(img, (x, y), (w, h), box_color, 2)\n",
    "\n",
    "    # Save the image with bounding boxes\n",
    "    if not os.path.exists(os.path.join(os.getcwd(), 'test_output')):\n",
    "        os.makedirs(os.path.join(os.getcwd(), 'test_output'))\n",
    "    # cv2.imshow(img)\n",
    "    img_path = f\"./test_output/output_image_{prefix}.png\"\n",
    "    cv2.imwrite(img_path, img)\n",
    "    return img_path\n",
    "\n",
    "def clean_targets(targets):\n",
    "    cleaned_targets = {}\n",
    "    # Filter out invalid boxes\n",
    "    valid_boxes_mask = (targets['boxes'].sum(axis=1) > 2)    \n",
    "    valid_labels_mask = (targets['labels'] != CLASSES_TO_IDX[\"background\"])\n",
    "    # valid_area_mask = (targets['area'] >= 1)\n",
    "\n",
    "    # Combine all conditions using \"&\"\n",
    "    final_mask = valid_boxes_mask & valid_labels_mask \n",
    "    # Apply the final mask\n",
    "    for key, target_tensor in targets.items():\n",
    "        if key == \"idx\":\n",
    "            cleaned_targets[key] = target_tensor\n",
    "            continue\n",
    "        cleaned_targets[key] = target_tensor[final_mask]\n",
    "    return cleaned_targets\n",
    "\n",
    "\n",
    "def inference_filter_prediction(outputs, iou_threshold=0.25, confidence_threshold=0.50):\n",
    "    cleaned_output = []\n",
    "    for predicted_dict in outputs:\n",
    "        mask = predicted_dict['scores'] >= confidence_threshold\n",
    "        filtered_detections = {k: v[mask] for k, v in predicted_dict.items()}\n",
    "        if len(filtered_detections['boxes'] != 0):\n",
    "            nms_indices = nms(\n",
    "                filtered_detections['boxes'],\n",
    "                filtered_detections['scores'],\n",
    "                iou_threshold\n",
    "            )\n",
    "            print(\"Before nms: \", len(filtered_detections[\"boxes\"]))\n",
    "            filtered_detections = {k: v[nms_indices] for k, v in filtered_detections.items()}\n",
    "            print(\"After nms: \", len(filtered_detections[\"boxes\"]))\n",
    "        cleaned_output.append(filtered_detections)\n",
    "    return cleaned_output\n",
    "\n",
    "\n",
    "def get_images(image_path):\n",
    "        # reading the images and converting them to correct size and color\n",
    "        original_image = cv2.imread(image_path)\n",
    "        grayscale = to_grayscale(original_image)\n",
    "        grayscale = normalize_image(grayscale)\n",
    "        grayscale = torch.from_numpy(grayscale).float()\n",
    "        grayscale = grayscale.unsqueeze(0)\n",
    "        \n",
    "        return grayscale, torch.from_numpy(original_image)\n",
    "\n",
    "def to_grayscale(image):\t\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return gray\n",
    "\n",
    "def normalize_image(img):\n",
    "    return img / 255\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, epoch):\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader, 1):\n",
    "        data = list(image.to(device) for image in data)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, targets)\n",
    "        print(f\"=====[ epoch {epoch} batch {batch_idx}  output of the model: {output}\")\n",
    "\n",
    "        loss = output[\"loss_classifier\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def run(model, train_loader):\n",
    "    epochs = 2\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, train_loader, optimizer, epoch)\n",
    "        scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL from volume: models/FasterRcnn_V1_epoch-6_model.pth is loaded successfuly\n",
      "MODEL from volume: models/FasterRcnn_V2_epoch-4_model.pth is loaded successfuly\n",
      "MODEL from volume: models/SSD_epoch-8_model.pth is loaded successfuly\n"
     ]
    }
   ],
   "source": [
    "model1 = load_model1()\n",
    "model2 = load_model2()\n",
    "model3 = get_ssd_detection_model()\n",
    "\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "\n",
    "original_image_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model1 filtering: \n",
      "Before nms:  31\n",
      "After nms:  20\n",
      "Model2 filtering: \n",
      "Before nms:  37\n",
      "After nms:  25\n",
      "Model3 filtering: \n",
      "Before nms:  200\n",
      "After nms:  53\n",
      "Time\n",
      "Model1 (Rcnn_V1): 0:00:00.913415\n",
      "Model2 (Rcnn_V2): 0:00:02.120378\n",
      "Model3 (SSD): 0:00:00.224682\n",
      "detections-203\n",
      "[{'boxes': tensor([[ 394.0913,  224.1995,  785.0116,  693.4707],\n",
      "        [ 600.5184,  383.9464,  960.0000, 1280.0001],\n",
      "        [ 410.5071,  440.4419,  714.4785, 1280.0001],\n",
      "        [ 646.6525,    0.0000,  960.0000,  312.9229],\n",
      "        [  13.4017,  351.1624,  385.5074, 1280.0001],\n",
      "        [ 104.0245,  144.2947,  476.9404,  536.2286],\n",
      "        [ 230.7604,    0.0000,  617.9042,  316.6730],\n",
      "        [   0.0000,  645.7056,  127.3112,  840.1622],\n",
      "        [ 521.5580,  801.0456,  731.2052, 1004.5587],\n",
      "        [   0.0000,  513.8107,  131.2592,  701.6232],\n",
      "        [ 643.7365,  656.8625,  865.9625,  852.7081],\n",
      "        [ 157.9126,  647.4512,  347.2741,  822.4556],\n",
      "        [   0.0000,    0.0000,  194.6432,  353.9010],\n",
      "        [   0.0000,  925.2112,  219.9171, 1280.0001],\n",
      "        [ 891.0008,    0.0000,  960.0000,  310.6226],\n",
      "        [ 960.0000,    0.0000,  960.0000,  512.8436],\n",
      "        [ 790.6820, 1015.8622,  960.0000, 1280.0001],\n",
      "        [ 463.8613, 1051.5260,  721.1686, 1280.0001],\n",
      "        [ 944.6414,  457.3759,  960.0000, 1113.1456],\n",
      "        [ 960.0000,   59.4119,  960.0000,  684.5297],\n",
      "        [ 249.8129, 1025.0576,  529.7767, 1280.0001],\n",
      "        [ 359.9872,  379.5193,  498.3988, 1238.1315],\n",
      "        [ 956.8356,  892.9135,  960.0000, 1280.0001],\n",
      "        [ 838.2119,  163.5956,  960.0000,  903.8004],\n",
      "        [ 272.5692,  228.4242,  428.8201, 1049.6034],\n",
      "        [ 960.0000,  713.5961,  960.0000, 1280.0001],\n",
      "        [ 960.0000,  572.3718,  960.0000, 1278.2690],\n",
      "        [ 649.8447,  539.9410,  861.6071,  708.6737],\n",
      "        [ 638.0013,  795.9879,  864.7970, 1026.4396],\n",
      "        [ 532.2497,  655.0484,  732.0980,  825.8577],\n",
      "        [   0.0000,  245.7642,  117.8634,  423.3509],\n",
      "        [ 346.6338,  504.4219,  538.6432,  660.7298],\n",
      "        [ 351.8679,  646.1661,  552.4528,  815.7957],\n",
      "        [   0.0000,  801.8903,  115.8777,  981.2305],\n",
      "        [ 581.2928,  943.5577,  765.3029, 1125.2640],\n",
      "        [ 134.0552,  816.5512,  322.7357,  973.8087],\n",
      "        [ 247.1307,  379.6375,  465.3367,  585.4524],\n",
      "        [ 464.2480,  926.7897,  662.9584, 1117.2642],\n",
      "        [ 634.1456,  396.7943,  852.4359,  567.2384],\n",
      "        [ 177.1854,  513.9532,  376.8363,  686.6677],\n",
      "        [   0.0000,  385.7900,  108.4720,  570.7087],\n",
      "        [ 160.7050,  954.6137,  325.4597, 1097.5443],\n",
      "        [ 536.4696,  520.1774,  732.2028,  668.3424],\n",
      "        [  65.0913,  399.0467,  265.4172,  571.6376],\n",
      "        [ 624.1511,  269.1451,  836.4142,  442.7231],\n",
      "        [ 323.9052,  802.8228,  552.6279,  970.5219],\n",
      "        [  76.7318,  523.9099,  255.3008,  691.2950],\n",
      "        [ 267.8329,  934.7701,  444.0940, 1092.5868],\n",
      "        [ 501.8656,  268.3978,  703.7468,  418.6295],\n",
      "        [ 510.7974,  392.2517,  712.1882,  548.4094],\n",
      "        [ 129.6148,  254.0764,  330.3893,  417.2061],\n",
      "        [ 444.1158,    0.0000,  749.5029,  213.8855],\n",
      "        [ 327.2933,  272.4801,  534.9935,  427.2435]],\n",
      "       grad_fn=<IndexBackward0>), 'scores': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<IndexBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 2, 4])}]\n",
      "{'boxes': tensor([[305.3919, 513.8285, 384.6668, 600.6637],\n",
      "        [318.7875, 380.6869, 359.3842, 410.6016],\n",
      "        [779.4629, 650.5868, 895.2635, 696.7663],\n",
      "        [835.0776, 689.7343, 955.0683, 766.3652],\n",
      "        [562.4211, 455.3195, 628.4827, 487.9072],\n",
      "        [765.0963, 487.5139, 825.0400, 526.2580],\n",
      "        [921.9033, 760.3970, 959.2503, 807.4948],\n",
      "        [364.9731, 704.9017, 591.7959, 971.8295],\n",
      "        [703.0774, 588.0437, 861.0406, 670.9641],\n",
      "        [662.3317, 431.0355, 729.4287, 461.8898],\n",
      "        [316.7217, 457.6261, 384.8579, 490.2362],\n",
      "        [205.9722, 285.1917, 226.2897, 305.2666],\n",
      "        [627.0372, 492.8382, 729.0192, 547.4794],\n",
      "        [822.7150, 485.0000, 873.6710, 527.7868],\n",
      "        [860.4413, 499.4603, 912.6068, 537.7047],\n",
      "        [619.4187, 455.1749, 671.8300, 495.3884],\n",
      "        [504.4633, 419.3502, 541.1194, 444.8752],\n",
      "        [543.7639, 437.3839, 556.2271, 472.7536],\n",
      "        [333.2316, 288.0610, 356.4151, 305.0683],\n",
      "        [226.9702, 267.8411, 248.2147, 281.6126]], grad_fn=<IndexBackward0>), 'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4]), 'scores': tensor([0.9542, 0.9492, 0.9237, 0.9019, 0.8866, 0.8416, 0.8342, 0.8320, 0.8275,\n",
      "        0.8192, 0.8035, 0.7992, 0.7646, 0.7112, 0.6453, 0.6338, 0.6259, 0.6198,\n",
      "        0.6164, 0.5397], grad_fn=<IndexBackward0>)}\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "{'boxes': tensor([[717.5181, 597.4793, 864.9887, 659.4029],\n",
      "        [301.0785, 517.4127, 391.3704, 599.2695],\n",
      "        [353.1155, 679.5331, 581.2241, 961.9435],\n",
      "        [621.2287, 491.5998, 735.2593, 549.1595],\n",
      "        [569.7141, 458.9854, 622.8361, 486.5567],\n",
      "        [834.8655, 690.3878, 960.0001, 765.4539],\n",
      "        [768.5941, 649.0569, 899.3454, 702.0298],\n",
      "        [762.7957, 473.9814, 831.6875, 521.1862],\n",
      "        [920.0439, 756.8998, 960.0001, 810.6301],\n",
      "        [316.4129, 446.1359, 388.3924, 495.4585],\n",
      "        [318.5170, 377.1647, 361.4722, 410.9377],\n",
      "        [860.1345, 495.0523, 915.8751, 537.8895],\n",
      "        [813.0851, 482.2948, 867.7799, 530.7085],\n",
      "        [395.4729, 451.0692, 419.7255, 474.5774],\n",
      "        [206.0785, 289.9435, 226.3158, 304.1769],\n",
      "        [664.4329, 418.2628, 744.2313, 466.3823],\n",
      "        [674.2969, 578.1944, 785.9300, 627.0538],\n",
      "        [493.5310, 418.6072, 541.2156, 446.3378],\n",
      "        [637.0562, 544.4656, 774.5404, 594.1846],\n",
      "        [622.3676, 456.2796, 670.4294, 496.4996],\n",
      "        [738.3197, 442.1815, 778.5788, 466.0373],\n",
      "        [541.8627, 437.9914, 557.9297, 470.4077],\n",
      "        [337.1946, 292.6483, 358.7595, 307.0360],\n",
      "        [774.6708, 455.8995, 821.5662, 484.4601],\n",
      "        [881.2494, 481.2463, 927.2579, 506.9978]], grad_fn=<IndexBackward0>), 'labels': tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4,\n",
      "        4]), 'scores': tensor([0.9391, 0.9262, 0.9215, 0.9159, 0.8990, 0.8872, 0.8790, 0.8785, 0.8616,\n",
      "        0.8461, 0.8418, 0.8364, 0.8201, 0.8161, 0.8123, 0.7839, 0.7645, 0.7632,\n",
      "        0.7500, 0.7354, 0.7353, 0.7056, 0.6664, 0.6543, 0.5222],\n",
      "       grad_fn=<IndexBackward0>)}\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(3)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "{'boxes': tensor([[ 394.0913,  224.1995,  785.0116,  693.4707],\n",
      "        [ 600.5184,  383.9464,  960.0000, 1280.0001],\n",
      "        [ 410.5071,  440.4419,  714.4785, 1280.0001],\n",
      "        [ 646.6525,    0.0000,  960.0000,  312.9229],\n",
      "        [  13.4017,  351.1624,  385.5074, 1280.0001],\n",
      "        [ 104.0245,  144.2947,  476.9404,  536.2286],\n",
      "        [ 230.7604,    0.0000,  617.9042,  316.6730],\n",
      "        [   0.0000,  645.7056,  127.3112,  840.1622],\n",
      "        [ 521.5580,  801.0456,  731.2052, 1004.5587],\n",
      "        [   0.0000,  513.8107,  131.2592,  701.6232],\n",
      "        [ 643.7365,  656.8625,  865.9625,  852.7081],\n",
      "        [ 157.9126,  647.4512,  347.2741,  822.4556],\n",
      "        [   0.0000,    0.0000,  194.6432,  353.9010],\n",
      "        [   0.0000,  925.2112,  219.9171, 1280.0001],\n",
      "        [ 891.0008,    0.0000,  960.0000,  310.6226],\n",
      "        [ 960.0000,    0.0000,  960.0000,  512.8436],\n",
      "        [ 790.6820, 1015.8622,  960.0000, 1280.0001],\n",
      "        [ 463.8613, 1051.5260,  721.1686, 1280.0001],\n",
      "        [ 944.6414,  457.3759,  960.0000, 1113.1456],\n",
      "        [ 960.0000,   59.4119,  960.0000,  684.5297],\n",
      "        [ 249.8129, 1025.0576,  529.7767, 1280.0001],\n",
      "        [ 359.9872,  379.5193,  498.3988, 1238.1315],\n",
      "        [ 956.8356,  892.9135,  960.0000, 1280.0001],\n",
      "        [ 838.2119,  163.5956,  960.0000,  903.8004],\n",
      "        [ 272.5692,  228.4242,  428.8201, 1049.6034],\n",
      "        [ 960.0000,  713.5961,  960.0000, 1280.0001],\n",
      "        [ 960.0000,  572.3718,  960.0000, 1278.2690],\n",
      "        [ 649.8447,  539.9410,  861.6071,  708.6737],\n",
      "        [ 638.0013,  795.9879,  864.7970, 1026.4396],\n",
      "        [ 532.2497,  655.0484,  732.0980,  825.8577],\n",
      "        [   0.0000,  245.7642,  117.8634,  423.3509],\n",
      "        [ 346.6338,  504.4219,  538.6432,  660.7298],\n",
      "        [ 351.8679,  646.1661,  552.4528,  815.7957],\n",
      "        [   0.0000,  801.8903,  115.8777,  981.2305],\n",
      "        [ 581.2928,  943.5577,  765.3029, 1125.2640],\n",
      "        [ 134.0552,  816.5512,  322.7357,  973.8087],\n",
      "        [ 247.1307,  379.6375,  465.3367,  585.4524],\n",
      "        [ 464.2480,  926.7897,  662.9584, 1117.2642],\n",
      "        [ 634.1456,  396.7943,  852.4359,  567.2384],\n",
      "        [ 177.1854,  513.9532,  376.8363,  686.6677],\n",
      "        [   0.0000,  385.7900,  108.4720,  570.7087],\n",
      "        [ 160.7050,  954.6137,  325.4597, 1097.5443],\n",
      "        [ 536.4696,  520.1774,  732.2028,  668.3424],\n",
      "        [  65.0913,  399.0467,  265.4172,  571.6376],\n",
      "        [ 624.1511,  269.1451,  836.4142,  442.7231],\n",
      "        [ 323.9052,  802.8228,  552.6279,  970.5219],\n",
      "        [  76.7318,  523.9099,  255.3008,  691.2950],\n",
      "        [ 267.8329,  934.7701,  444.0940, 1092.5868],\n",
      "        [ 501.8656,  268.3978,  703.7468,  418.6295],\n",
      "        [ 510.7974,  392.2517,  712.1882,  548.4094],\n",
      "        [ 129.6148,  254.0764,  330.3893,  417.2061],\n",
      "        [ 444.1158,    0.0000,  749.5029,  213.8855],\n",
      "        [ 327.2933,  272.4801,  534.9935,  427.2435]],\n",
      "       grad_fn=<IndexBackward0>), 'scores': tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
      "       grad_fn=<IndexBackward0>), 'labels': tensor([1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
      "        5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 2, 4])}\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(5)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(4)\n",
      "tensor(2)\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "image_dir = \"./real_images/simple.jpg\"\n",
    "grayscale, image = get_images(image_dir)\n",
    "grayscale = grayscale.unsqueeze(0)\n",
    "inputs = grayscale\n",
    "\n",
    "timer_model1 = datetime.datetime.now()\n",
    "outputs1 = model1(inputs)\n",
    "timer_model1 = datetime.datetime.now() - timer_model1 \n",
    "\n",
    "# print(f\"outputs1: {outputs1}\")\n",
    "timer_model2 = datetime.datetime.now()\n",
    "outputs2 = model2(inputs)\n",
    "timer_model2 = datetime.datetime.now() - timer_model2\n",
    "\n",
    "timer_model3 = datetime.datetime.now()\n",
    "outputs3 = model3(inputs)\n",
    "timer_model3 = datetime.datetime.now() - timer_model3\n",
    "\n",
    "\n",
    "# print(f\"outputs1: {outputs1}\")\n",
    "print(\"Model1 filtering: \")\n",
    "outputs1 = inference_filter_prediction(outputs1)\n",
    "print(\"Model2 filtering: \")\n",
    "outputs2 = inference_filter_prediction(outputs2)\n",
    "print(\"Model3 filtering: \")\n",
    "outputs3 = inference_filter_prediction(outputs3)\n",
    "print(f\"Time\\nModel1 (Rcnn_V1): {timer_model1}\\nModel2 (Rcnn_V2): {timer_model2}\\nModel3 (SSD): {timer_model3}\")\n",
    "\n",
    "it = random.randint(0, 1000)\n",
    "prefix=f\"detections-{it}\"\n",
    "print(prefix)\n",
    "print(outputs3)\n",
    "\n",
    "for output1, output2, output3 in zip(outputs1, outputs2,outputs3):\n",
    "    save_test_img(torch.clone(image), output1, f\"model1_{prefix}\")\n",
    "    save_test_img(torch.clone(image), output2, f\"model2_{prefix}\")\n",
    "    save_test_img(image, output3, f\"model3_{prefix}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
